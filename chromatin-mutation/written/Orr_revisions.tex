\input{header}
\bibliography{comps_revisions.bib}

%add line at half page for reference
\usepackage{eso-pic}
\AddToShipoutPictureBG{%
    \AtTextCenter{\hspace{-0.5\textwidth}\rule{\textwidth}{0.5pt}}}

\begin{document}

% As your proposed dissertation project focuses on error profiles of NGS alignments, briefly explain how the most widely-used error correction algorithms work (i.e., k-spectrum-based approaches vs suffix-tree/array-based methods), thereby highlighting their differences/advantages/disadvantages.

%methods assume errors are rare and random

%================
%k-spectrum-based
%================
% a table of k-mers and the number of times they occur in the read set is created
% some count threshold is calculated to distinguish between low frequency "untrusted" kmers versus high frequency "trusted" kmers
% the calculation of this threshold differs in different implementations.
% reads containing an untrusted kmer are repaired with the minimum number of changes to turn all its kmers into trusted kmers.

%----------------
% * advantages
%----------------
% low memory requirement

%----------------
% * disadvantages
%----------------
% new data structure must be created for every value of k

%================
%suffix-tree/array-based
%================
% suffix tree is data structure holding every possible suffix from every read in the read set, along with how frequent each suffix is.
% the tree is searched for low frequency nodes, which represent infrequent suffixes due to errors.

%suffix array attempts to reduce the memory requirements of a full suffix tree by storing only an array of all possible suffixes along with an array of the longest common prefix of the previous suffix.

%----------------
% * advantages
%----------------
% can quickly get k-mer frequencies for any k from only one structure
% allows rapid optimization of k

%----------------
% * disadvantages
%----------------
% requires a lot of memory

%================
%major differences between the two method types
%================

%both call 


\section{Question 1}
Error correction methods make two assumptions to detect and correct errors in NGS reads: 1) errors are rare and 2) errors are random. Under these assumptions, errors can be corrected by looking at every called base sequenced from the same true base and changing those that occur with sufficiently small frequency to one with higher frequency.
To do this, the bases at each position must be piled up to determine the frequency of each nucleotide.

To make the methods independent of a reference genome, which may lead to improper corrections due to alignment errors, the substrings of reads are compared to the substrings present in other reads to estimate the frequency of each called nucleotide at each site.
Substrings that likely do not contain errors are called "trusted", and those that likely do are called "untrusted".
Different methods for error correction differ in how the substrings are compared, the data structures and compression methods used to store the substring information, and the statistics used to determine when a substring is trusted or untrusted.
Recently developed error correction methods fall into one of two major categories: \textit{k}-spectrum methods and suffix-tree/array methods.

\textit{k}-spectrum error correction methods split each read in the dataset into its component \textit{k}-mers, or substrings of length \textit{k}. The number of occurences of each \textit{k}-mer is counted and stored, along with the reads that contain that \textit{k}-mer. Note that this must be done for a user-defined value of \textit{k}, and if the user wishes to change this value, the table creation process must be repeated.
The frequency of each \textit{k}-mer estimates the sequencing depth at the sites that \textit{k}-mer covers. Thus, \textit{k}-mers that contain errors will occur very infrequently compared to those without errors.
The distribution of \textit{k}-mer counts generated this way is generally bimodal, with erroneous \textit{k}-mers forming a peak at very low counts, a peak representing true \textit{k}-mers at higher counts, and possibly further peaks representing \textit{k}-mers from regions of repetitive DNA. \textit{k}-spectrum methods determine a count treshold that reads must exceed to be considered trusted.

Different methods determine this cutoff differently; for example,
% EULER-SR \parencite{chaisson_short_2008} estimates the average coverage, models that with a Poisson distribution, and sets the cutoff such that only a small percentage of correct \textit{k}-mers are untrusted.
ALLPATHS \parencite{butler_allpaths:_2008} uses the two modes present in the \textit{k}-mer count distribution to model a Poisson distribution for untrusted \textit{k}-mers and a separate model for trusted \textit{k}-mers. Under this model, the cutoff that captures the most erroneous \textit{k}-mers and the least correct \textit{k}-mers is the smallest count with the lowest frequency that appears between the two peaks.

% Rcorrector, used the proposed pipeline, is a \textit{k}-spectrum approach that uses a read-specific and \textit{k}-mer specific threshold to avoid mistakenly classifying low-coverage \textit{k}-mers as erroneous. Rcorrector takes a subset of high-frequency \textit{k}-mers and calculates the ratio of the highest and second-highest counts of the \textit{k}-mer following it in a De Bruijn graph of the reads. It then chooses the smallest ratio larger than 95\% of these ratios as the global parameter $\alpha$.
% Let $t$ be the largest count of the \textit{k}-mer following the questionable \textit{k}-mer in the De Bruijn graph.
% The \textit{k}-mer is untrusted if its count is less than $g(t) = \alpha t + 6\sqrt{\alpha t}$.
% This means the threshold is chosen based on the ability to distinguish the \textit{k}-mer from one containing an error, which is expressed in $\alpha$.
% Additionally, any \textit{k}-mer in a read is untrusted if its count is less than the value of $g$ for the first \textit{k}-mer in that read that is half as frequent as the previous \textit{k}-mer when the \textit{k}-mers of the read are ranked by decreasing frequency. 

After classifying \textit{k}-mers as trusted or untrusted, those that are untrusted are converted to those that are trusted to repair the error. If a read contains too many errors, it is discarded. Errors are repaired by making the minimum number of changes to bases in the read such that all the untrusted \textit{k}-mers become trusted.

Suffix-tree and suffix-array based approaches use the set of all suffixes of all reads, rather than substrings of fixed length, to detect and repair errors. Suffix trees are trees where each node represents a nucleotide and each edge represents the next nucleotide in one of the suffixes. Since this tree represents every possible substring in every read in the data set, it uses more memory than the \textit{k}-mer count tables used in \textit{k}-spectrum approaches. However, the data structure only needs to be calculated once, even if multiple substring sizes are considered when correcting the reads.

In a suffix tree, the number of occurrences of the substring given by concatenating the characters from the root to an internal node is equal to the number of leaves of the subtree rooted at that node. The level in the tree of this node is equivalent a \textit{k}-mer, with \textit{k} equal to the depth of the node. The expected number of occurrences of the substring is binomially distributed; if this actual number of occurrences is lower than a user-defined cutoff, the base represented by the node is considered erroneous. If the sibling of the erroneous node is error-free, the subtrees can be merged, and all reads changed to reflect the correct base. This method is implemented in the SHREC package \parencite{schroder_shrec:_2009}.

The suffix array is similar to the suffix tree, but uses less memory. Rather than storing the entire tree, an array of suffixes is created along with an array of common prefixes. If an error occurs at the first base in a suffix, it will have the same prefix as the non-erroneous base, but will occur less frequently. Suffix arrays have much of the same functionality as suffix trees, but use less memory.
Thus, \textit{k}-spectrum methods use less memory but can only consider substrings of a single value while suffix tree and suffix array methods use more memory but can optimize correction using multiple substring lengths.

% Address the issue of variation in coverage, specifically, how this could confound your analyses and how to correct for it.
\section{Question 2}
Random variation in coverage can affect my analysis by contributing to mistakes in error correction, inaccuracy in determining the degree of chromatin accessibility, and inaccuracy in mutation detection. During error correction, true variation that is infrequently sampled can be mistaken for a sequencing error. During ATAC-seq peak calling, random variation in coverage can cause over or underestimation of the true accessibility at a site. During mutation detection, low coverage can cause sequencing errors to seem like true mutations and failure to detect true mutations because they weren't sampled. Errors in ATAC-seq peak calling and mutation detection will result in incorrect conclusions during data analysis. Therefore, the software used in the pipeline will be chosen to minimize the impact of variable coverage on error correction, controls will be used for the chromatin accessibility assay, and pipeline validation will be used to detect when coverage adversely impacts the variant calling accuracy of the pipeline. Regions where coverage is too low will then be excluded from the analysis.

There are many different methods available for error correction. Most of these methods rely on having relatively consistent coverage to correctly estimate when a base call occurs at low enough frequency to be considered an error. This becomes problematic when coverage is variable, and infrequently sampled variation can be mistakenly identified as an error. However, the error correction software used in this analysis, Rcorrector \parencite{song_rcorrector:_2015}, is designed for use in RNA-seq and therefore attempts to be robust to variation in coverage.
It does this by defining a global threshold for \textit{k}-mer frequency while also defining a read-level threshold.

Briefly, Rcorrector works by taking a subset of the highest frequency \textit{k}-mers and defining a \textit{k}-mer threshold based on the ratio of these \textit{k}-mers to the frequency of a \textit{k}-mer that is different by one nucleotide. The read-level threshold is calculated by ranking the counts of the \textit{k}-mers in the read; if there is a two-fold change in the count of this list of \textit{k}-mers, the threshold of that \textit{k}-mer is used as the read-level threshold. Any read with a \textit{k}-mer that has a count lower than the \textit{k}-mer threshold or the read-level threshold is marked for correction. Ultimately, this approach helps prevent false positive error detections in regions sequenced at low coverage.

For accurately detecting ATAC-seq peaks, a sequencing control is used.
Regions are only determined to be significantly accessible if the null hypothesis that the number of ATAC-seq reads is equal to the number of reads derived from the standard whole-genome sequencing library.
For this analysis, the ATAC-seq reads are normalized by the number of reads in the sequencing control.
If some confounding factor that increases sequencing coverage in a particular region, it would be enhanced in both the control and ATAC-seq reads. This means that the enrichment of ATAC-seq reads relative to the control reads would not be significantly impacted.

Variation in coverage would likely have the largest impact on the ability to call mutations in low-coverage regions. For this reason, many people use a coverage cutoff, and only attempt to analyze regions that exceed a particular coverage threshold. Part of the development of the pipeline involves sequencing a heterogeneous mixture of two cell lines with known genotypes. The "mutations" called from sequencing this mixture are therefore from the cell line that makes up the minor fraction of the mixture, and true mutations will match the genotype of this cell line. The accuracy of the pipeline will be evaluated by comparing the called mutations with the true genotype of this cell line. The accuracy of the pipeline at different coverage levels can then be determined, and the level of coverage at which the pipeline begins to perform poorly can be used as the coverage cutoff. Regions that do not have sufficient coverage for accurate mutation detection can then be excluded from the analysis.


% Describe the details of how biological variation (e.g., a diploid versus a non-diploid genome; inbred vs outbred; types of tissues compared) might affect the components of your pipeline (e.g. the QC part - FastQC, Timmomatic, and the variant calling/error correction), given the algorithms on which they are based work.
\section{Question 3}


% Describe the model used by MuTect2 to genotype somatic mutations including (1) assumptions made, (2) parameters used, (3) calculations performed, (4) choice of default parameters, and (5) metrics used to call a mutation or not.
\section{Question 4}

MuTect2 \parencite{cibulskis_sensitive_2013} is an adaptation of the MuTect SNP caller that performs local realignment like the Genome Analysis Toolkit's (GATK) HaplotypeCaller germline variant caller to call insertions and deletions. It is designed to detect mutations in cancer samples and to be robust to heterogeneity by detecting mutations that occur at low sample frequency. The inputs of MuTect2 are: a BAM file of aligned tumor reads, a BAM file containing aligned reads from a matched normal sample, and optionally a VCF file containing variants to white-list (such as from the COSMIC database) or variants to black-list (such as from dbSNP). 

The first assumptions MuTect2 makes is that optical duplicates in the BAM file have been marked and base quality scores are recalibrated. This means that the reads should be fairly independent and the quality scores should somewhat accurately reflect the probability that the sequenced base is an error.

\end{document}
