\input{header}
\bibliography{comps_revisions.bib}

%add line at half page for reference
\usepackage{eso-pic}
\AddToShipoutPictureBG{%
    \AtTextCenter{\hspace{-0.5\textwidth}\rule{\textwidth}{0.5pt}}}

\begin{document}

% As your proposed dissertation project focuses on error profiles of NGS alignments, briefly explain how the most widely-used error correction algorithms work (i.e., k-spectrum-based approaches vs suffix-tree/array-based methods), thereby highlighting their differences/advantages/disadvantages.

%methods assume errors are rare and random

%================
%k-spectrum-based
%================
% a table of k-mers and the number of times they occur in the read set is created
% some count threshold is calculated to distinguish between low frequency "untrusted" kmers versus high frequency "trusted" kmers
% the calculation of this threshold differs in different implementations.
% reads containing an untrusted kmer are repaired with the minimum number of changes to turn all its kmers into trusted kmers.

%----------------
% * advantages
%----------------
% low memory requirement

%----------------
% * disadvantages
%----------------
% new data structure must be created for every value of k

%================
%suffix-tree/array-based
%================
% suffix tree is data structure holding every possible suffix from every read in the read set, along with how frequent each suffix is.
% the tree is searched for low frequency nodes, which represent infrequent suffixes due to errors.

%suffix array attempts to reduce the memory requirements of a full suffix tree by storing only an array of all possible suffixes along with an array of the longest common prefix of the previous suffix.

%----------------
% * advantages
%----------------
% can quickly get k-mer frequencies for any k from only one structure
% allows rapid optimization of k

%----------------
% * disadvantages
%----------------
% requires a lot of memory

%================
%major differences between the two method types
%================

%both call 


\section{Question 1}
Error correction methods make two assumptions to detect and correct errors in NGS reads: 1) errors are rare and 2) errors are random. Under these assumptions, errors can be corrected by looking at every called base sequenced from the same true base and changing those that occur with sufficiently small frequency to one with higher frequency.
To do this, the bases at each position must be piled up to determine the frequency of each nucleotide.

To make the methods independent of a reference genome, which may lead to improper corrections due to alignment errors, the substrings of reads are compared to the substrings present in other reads to estimate the frequency of each called nucleotide at each site.
Substrings that likely do not contain errors are called "trusted", and those that likely do are called "untrusted".
Different methods for error correction differ in how the substrings are compared, the data structures and compression methods used to store the substring information, and the statistics used to determine when a substring is trusted or untrusted.
Recently developed error correction methods fall into one of two major categories: \textit{k}-spectrum methods and suffix-tree/array methods.

\textit{k}-spectrum error correction methods split each read in the dataset into its component \textit{k}-mers, or substrings of length \textit{k}. The number of occurences of each \textit{k}-mer is counted and stored, along with the reads that contain that \textit{k}-mer. Note that this must be done for a user-defined value of \textit{k}, and if the user wishes to change this value, the table creation process must be repeated.
The frequency of each \textit{k}-mer estimates the sequencing depth at the sites that \textit{k}-mer covers. Thus, \textit{k}-mers that contain errors will occur very infrequently compared to those without errors.
The distribution of \textit{k}-mer counts generated this way is generally bimodal, with erroneous \textit{k}-mers forming a peak at very low counts, a peak representing true \textit{k}-mers at higher counts, and possibly further peaks representing \textit{k}-mers from regions of repetitive DNA. \textit{k}-spectrum methods determine a count treshold that reads must exceed to be considered trusted.

Different methods determine this cutoff differently; for example,
% EULER-SR \parencite{chaisson_short_2008} estimates the average coverage, models that with a Poisson distribution, and sets the cutoff such that only a small percentage of correct \textit{k}-mers are untrusted.
ALLPATHS \parencite{butler_allpaths:_2008} uses the two modes present in the \textit{k}-mer count distribution to model a Poisson distribution for untrusted \textit{k}-mers and a separate model for trusted \textit{k}-mers. Under this model, the cutoff that captures the most erroneous \textit{k}-mers and the least correct \textit{k}-mers is the smallest count with the lowest frequency that appears between the two peaks.

% Rcorrector, used the proposed pipeline, is a \textit{k}-spectrum approach that uses a read-specific and \textit{k}-mer specific threshold to avoid mistakenly classifying low-coverage \textit{k}-mers as erroneous. Rcorrector takes a subset of high-frequency \textit{k}-mers and calculates the ratio of the highest and second-highest counts of the \textit{k}-mer following it in a De Bruijn graph of the reads. It then chooses the smallest ratio larger than 95\% of these ratios as the global parameter $\alpha$.
% Let $t$ be the largest count of the \textit{k}-mer following the questionable \textit{k}-mer in the De Bruijn graph.
% The \textit{k}-mer is untrusted if its count is less than $g(t) = \alpha t + 6\sqrt{\alpha t}$.
% This means the threshold is chosen based on the ability to distinguish the \textit{k}-mer from one containing an error, which is expressed in $\alpha$.
% Additionally, any \textit{k}-mer in a read is untrusted if its count is less than the value of $g$ for the first \textit{k}-mer in that read that is half as frequent as the previous \textit{k}-mer when the \textit{k}-mers of the read are ranked by decreasing frequency. 

After classifying \textit{k}-mers as trusted or untrusted, those that are untrusted are converted to those that are trusted to repair the error. If a read contains too many errors, it is discarded. Errors are repaired by making the minimum number of changes to bases in the read such that all the untrusted \textit{k}-mers become trusted.

Suffix-tree and suffix-array based approaches use the set of all suffixes of all reads, rather than substrings of fixed length, to detect and repair errors. Suffix trees are trees where each node represents a nucleotide and each edge represents the next nucleotide in one of the suffixes. Since this tree represents every possible substring in every read in the data set, it uses more memory than the \textit{k}-mer count tables used in \textit{k}-spectrum approaches. However, the data structure only needs to be calculated once, even if multiple substring sizes are considered when correcting the reads.

In a suffix tree, the number of occurrences of the substring given by concatenating the characters from the root to an internal node is equal to the number of leaves of the subtree rooted at that node. The level in the tree of this node is equivalent a \textit{k}-mer, with \textit{k} equal to the depth of the node. The expected number of occurrences of the substring is binomially distributed; if this actual number of occurrences is lower than a user-defined cutoff, the base represented by the node is considered erroneous. If the sibling of the erroneous node is error-free, the subtrees can be merged, and all reads changed to reflect the correct base. This method is implemented in the SHREC package \parencite{schroder_shrec:_2009}.

The suffix array is similar to the suffix tree, but uses less memory. Rather than storing the entire tree, an array of suffixes is created along with an array of common prefixes. If an error occurs at the first base in a suffix, it will have the same prefix as the non-erroneous base, but will occur less frequently. Suffix arrays have much of the same functionality as suffix trees, but use less memory.
Thus, \textit{k}-spectrum methods use less memory but can only consider substrings of a single value while suffix tree and suffix array methods use more memory but can optimize correction using multiple substring lengths.

% Address the issue of variation in coverage, specifically, how this could confound your analyses and how to correct for it.
\section{Question 2}
Random variation in coverage can affect my analysis by contributing to mistakes in error correction, inaccuracy in determining the degree of chromatin accessibility, and inaccuracy in mutation detection. During error correction, true variation that is infrequently sampled can be mistaken for a sequencing error. During ATAC-seq peak calling, random variation in coverage can cause over or underestimation of the true accessibility at a site. During mutation detection, low coverage can cause sequencing errors to seem like true mutations and failure to detect true mutations because they weren't sampled. Errors in ATAC-seq peak calling and mutation detection will result in incorrect conclusions during data analysis. Therefore, the software used in the pipeline will be chosen to minimize the impact of variable coverage on error correction, controls will be used for the chromatin accessibility assay, and pipeline validation will be used to detect when coverage adversely impacts the variant calling accuracy of the pipeline. Regions where coverage is too low will then be excluded from the analysis.

There are many different methods available for error correction. Most of these methods rely on having relatively consistent coverage to correctly estimate when a base call occurs at low enough frequency to be considered an error. This becomes problematic when coverage is variable, and infrequently sampled variation can be mistakenly identified as an error. However, the error correction software used in this analysis, Rcorrector \parencite{song_rcorrector:_2015}, is designed for use in RNA-seq and therefore attempts to be robust to variation in coverage.
It does this by defining a global threshold for \textit{k}-mer frequency while also defining a read-level threshold.

Briefly, Rcorrector works by taking a subset of the highest frequency \textit{k}-mers and defining a \textit{k}-mer threshold based on the ratio of these \textit{k}-mers to the frequency of a \textit{k}-mer that is different by one nucleotide. The read-level threshold is calculated by ranking the counts of the \textit{k}-mers in the read; if there is a two-fold change in the count of this list of \textit{k}-mers, the threshold of that \textit{k}-mer is used as the read-level threshold. Any read with a \textit{k}-mer that has a count lower than the \textit{k}-mer threshold or the read-level threshold is marked for correction. Ultimately, this approach helps prevent false positive error detections in regions sequenced at low coverage.

For accurately detecting ATAC-seq peaks, a sequencing control is used.
Regions are only determined to be significantly accessible if the null hypothesis that the number of ATAC-seq reads is equal to the number of reads derived from the standard whole-genome sequencing library.
For this analysis, the ATAC-seq reads are normalized by the number of reads in the sequencing control.
If some confounding factor that increases sequencing coverage in a particular region, it would be enhanced in both the control and ATAC-seq reads. This means that the enrichment of ATAC-seq reads relative to the control reads would not be significantly impacted.

Variation in coverage would likely have the largest impact on the ability to call mutations in low-coverage regions. For this reason, many people use a coverage cutoff, and only attempt to analyze regions that exceed a particular coverage threshold. Part of the development of the pipeline involves sequencing a heterogeneous mixture of two cell lines with known genotypes. The "mutations" called from sequencing this mixture are therefore from the cell line that makes up the minor fraction of the mixture, and true mutations will match the genotype of this cell line. The accuracy of the pipeline will be evaluated by comparing the called mutations with the true genotype of this cell line. The accuracy of the pipeline at different coverage levels can then be determined, and the level of coverage at which the pipeline begins to perform poorly can be used as the coverage cutoff. Regions that do not have sufficient coverage for accurate mutation detection can then be excluded from the analysis.


% Describe the details of how biological variation (e.g., a diploid versus a non-diploid genome; inbred vs outbred; types of tissues compared) might affect the components of your pipeline (e.g. the QC part - FastQC, Timmomatic, and the variant calling/error correction), given the algorithms on which they are based work.
\section{Question 3}
%ploidy differences
%heterozygosity (inbred v outbred)
%tissue type compared (heterogeneity, amount of true variation detectable)

%fastqc
%trimmomatic
%error correction - Rcorrector
%error correction - Nanocorr
%variant calling - MuTect2 (triallelic sites problematic)
%variant calling - Nanopolish
%accessibility - MACS

Overall, any source of excess biological variation increases the likelihood of error during the analysis, as biological variation makes somatic variation more difficult to distinguish from this background variation and sequencing error. Most steps in the pipeline should be fairly robust to biological variation, but others are more affected.

The read quality control steps will be least impacted by biological variation. FastQC checks the sequencing reads for adapters, overrepresented \textit{k}-mers, flowcell-related quality issues, low per-base quality scores, low mean quality scores, bias in sequenced base per site, biased GC content, high read N content, missing reads, an unusually large proportion of duplicated reads, and an unusually large number of duplications of a single read. These checks will not significantly depend on biological variation except the potential for overrepresented \textit{k}-mers or duplicate reads that can result from genomes with a large number of duplications. Trimmomatic is likely not significantly affected either; it searches for adapter sequences present in the reads and removes them. Additionally, Trimmomatic scans forward and reverse reads for evidence of adapter sequences caused by short insert sizes, causing the sequence of the opposite adapter to be present in the forward and reverse reads. This will not be significantly impacted by biological variation.

% FastQC and Trimmomatic can both falsely detect reads  if the sample contains DNA similar to Illumina adapters. This shouldn't be an issue, since the adapters are designed to be distinct from natural sequences, but should be considered.

On the other hand, the read correction steps will be impacted by biological variation. Rcorrector uses \textit{k}-mer frequencies in reads to identify low frequency \textit{k}-mers that may be errors and correct them. Though Rcorrector is designed to work even with nonuniform coverage, it may be affected by nonuniform coverage that results from changes in ploidy. Increased heterozygosity in outbred strains may also affect this, as one allele may be sampled less frequently than the other, making that allele erroneously appear as an error, increasing the false negative rate of the mutation detection component of the pipeline. True heterogeneity present in the sampled tissue would affect error correction similar to increased heterozygosity; infrequently sampled variants would not be detected, and may possibly be counted as evidence for a different nucleotide. For small amounts of heterogeneity that would not significantly impact the conclusions; however, for high levels of tissue heterogeneity, the effect would be significant. Errors made during Illumina error correction would then be propagated to the nanopore reads when corrected using Nanocorr, as Nanocorr uses the consensus of the short read sequences to edit the long reads.

%MuTect2 and Nanopolish
The variant detection step will be the component of the pipeline most affected by biological variation. Variant detection will suffer due to high ploidy, high amounts of heterozygosity, and choice of compared tissues.

%MACS2

% Describe the model used by MuTect2 to genotype somatic mutations including (1) assumptions made, (2) parameters used, (3) calculations performed, (4) choice of default parameters, and (5) metrics used to call a mutation or not.
\section{Question 4}

MuTect2 \parencite{cibulskis_sensitive_2013} is an adaptation of the MuTect SNP caller that performs local realignment like the Genome Analysis Toolkit's (GATK) HaplotypeCaller germline variant caller to call insertions and deletions. It is designed to detect mutations in cancer samples and to be robust to heterogeneity by detecting mutations that occur at low sample frequency. The inputs of MuTect2 are: a BAM file of aligned tumor reads, a BAM file containing aligned reads from a matched normal sample, and optionally a VCF file containing variants to white-list (such as from the COSMIC database) or variants to black-list (such as from dbSNP). 

The first assumptions MuTect2 makes is that optical duplicates in the BAM file have been marked and base quality scores are recalibrated. This means that the reads should be fairly independent and the quality scores should accurately reflect the probability that the sequenced base is an error. Additionally, MuTect2 assumes all sequencing errors are independent across reads and substitutions of each type occur with equal probability.
The original implementation of MuTect also made the assumption that the read was correctly aligned to the reference; however, MuTect2 realigns reads in a region before calling and emitting variants using the algorithm implemented in the GATK HaplotypeCaller tool. The genotyping model used by MuTect2 then assumes reads are properly mapped.

% It does this by building a De Bruijn graph of the reads in a region to reassemble it. Two graphs are constructed: one using \textit{k}-mers of size 10 and another using \textit{k}-mers of size 25. If a cycle is formed in the De Bruijn graph or the number of unique \textit{k}-mers is greater than 25\% of \textit{k}-mers in the graph, the largest \textit{k} is incremented by 10, and another attempt to form the graph is made. If this fails 6 times, the region is ignored.

% It also traverses this graph searching for divergent paths that represent haplotypes present in the sample. These haplotype sequences are then realigned to the reference using the Smith-Waterman algorithm, and the reads altered to include gaps.

MuTect2 considers many more potential mutations than it emits. To decide whether a site contains a putative mutation, MuTect2 calculates the log ratio of likelihoods of a model containing a mutation at frequency $f$ and a model where no mutation occurred (that is, $f = 0$). The likelihood of a mutation $m$ occurring at frequency $f$ with reference allele $r$ sequenced at depth $d$ with bases $b_i$ and probability of error $e_i$ is $L(M_f^m)=P({b_i}|{e_i},r,m,f)=\prod_{i=1}^{d} P(b_i|e_i,r,m,f)$ and 

$$
P(b_i|e_i,r,m,f)=
\begin{cases}
f^{e_i/3}+(1-f)(1-e_i) & \text{if } b_i=r \\
f(1-e_i) + (1-f)^{e_i/3} & \text{if } b_i=m \\
e_i/3 & \text{otherwise}
\end{cases}
$$

Here, $f$ is estimated to be $\frac{\text{number of reads matching }m}{d}$.
If the log ratio of this model exceeds a threshold $\delta{}_T$, that is: $LOD_T(m,f)=\log_{10} \left(\frac{L(M_f^m)P(m,f)}{L(M_0)(1-P(m,f))}\right)$ , then $m$ is considered a candidate variant. This is simplified using the assumptions that mutations are uniform so $P(f)=1$, and that $P(m)$ is independent from $P(f)$. Thus, a mutation is considered if $LOD_T(m,f)=\log_{10} \left(\frac{L(M_f^m)}{L(M_0)}\right) \geq \log_{10} \delta{} - \log_{10}  \left(\frac{P(m)}{1-P(m)}\right) = \theta{}_T$. MuTect2 assumes a typical tumor frequency is $3 \times 10^{-6}$, so the default value of $\theta{}_T$ is 6.3. This parameter can be changed during invocation of the software.

Potential mutations that exceed this threshold are then filtered. MuTect2 rejects variants that fall within 5 base-pairs of an insertion or deletion supported by 3 or more reads. If over half the reads that cover the site have a mapping quality of 0, the mutation is rejected. If there is not a read that covers the site and supports the mutation with a mapping quality score above 20, the mutation is rejected. Triallelic sites are also rejected. To determine whether a false positive is due to a strand-biased sequencing error, the reads overlapping a potential mutation are split into two subsets, one for each direction. The LOD score is then recalculated for the two sets; if this score is < 2 in either direction while the subset still maintains over 90\% sensitivity, the mutation is rejected. The sensitivity to detect a mutation is given by $\sum_{i=k}^d \text{binom}(i|d,f(1-e)+(1-f)e)$, where $k$ is the minimum number of reads supporting a mutant allele required such that the LOD score is greater than $\theta{}_T$. Candidate mutations are also removed if the median distance between the mutation and the start or end of the read is less than or equal to 10, or if the median absolute deviation from the start or end of the read is less than or equal to 3. Finally, a variant is rejected if it appears too many times in the control sample (2 or more times or over 3\% of the reads) and the sum of the quality scores of those reads is over 20.

Mutations that pass filtering are categorized as somatic or germline and then emitted. To do so, a LOD score is calculated as above; however, this time the model that the variant does not exist is tested against the model that the the variant is a heterozygous germline variant; that is, $LOD_N=\log_{10} \left(\frac{L(M_0)P(m,f)}{L(M_{0.5}^m)P(\text{germ line})}\right) \geq \log_{10} \delta{}_N$. If this is the case, the mutation is more likely to be somatic than germline. By default, MuTect2 sets a high threshold to avoid misclassifying germline events as somatic. To do so, it sets $\delta{}_N = 10$ to calculate the default threshold $\theta{}_N$. Rewriting the LOD equation as $LOD_N = \log_{10} \left(\frac{L(M_0)}{L(M_{0.5}^m)}\right) \geq \log_{10} \delta{}_N - \log_{10} \left(\frac{P(m)}{P(\text{germ line})}\right) = \theta{}_N$. $P(m)$ is calculated as above; to calculate $P(\text{germline})$, MuTect2 considers two categories of sites: known variants in dbSNP, and other sites. There are about $30 \times 10^6$ variant sites in dbSNP, which is on average 1000 variants/Mb. An estimated 5\% of an individual's variants are not in dbSNP; thus, P(germline|non-dbSNP site) = $5 \times 10^{-5}$ and P(germline|dbSNP site) = $0.095$. This gives two thresholds for categorizing variants: $\theta{}_{N|non-dbSNP site} = 2.2$ and $\theta{}_{N|dbSNP site} = 5.5$

\end{document}
